---
publication_types:
  - "1"
authors:
  - Rui Li
  - Xiantuo He
  - Danna Xue
  - Shaolin Su
  - Qing Mao
  - Yu Zhu
  - Jinqiu Sun
  - Yanning Zhang
publication_short: #(Will be available soon)
abstract: Self-supervised depth estimation has made a great success in learning
  depth from unlabeled image sequences. While the mappings between image and
  pixel-wise depth are well-studied in current methods, the correlation between
  image, depth and scene semantics, however, is less considered. This hinders
  the network to better understand the real geometry of the scene, since the
  contextual clues, contribute not only the latent representations of scene
  depth, but also the straight constraints for depth map. In this paper, we
  leverage the two benefits by proposing the implicit and explicit semantic
  guidance for accurate self-supervised depth estimation. We propose a
  Semantic-aware Spatial Feature Alignment (SSFA) scheme to effectively align
  implicit semantic features with depth features for scene-aware depth
  estimation. We also propose a semantic-guided ranking loss to explicitly
  constrain the estimated depth maps to be consistent with real scene contextual
  properties. Both semantic label noise and prediction uncertainty is considered
  to yield reliable depth supervisions. Extensive experimental results show that
  our method produces high quality depth maps which are consistently superior
  either on complex scenes or diverse semantic categories, and outperforms the
  state-of-the-art methods by a significant margin.
draft: false
url_pdf: "https://arxiv.org/abs/2102.06685"
url_dataset: ""
url_project: ""
url_source: ""
url_video: ""
title: "Learning Depth via Leveraging Semantics: Self-supervised Monocular Depth
  Estimation with Both Implicit and Explicit Semantic Guidance"
publication: arXiv preprint
featured: false
image:
  filename: featured.jpg
  focal_point: ""
  preview_only: false
date: 2021-02-01T04:00:00.000Z
url_slides: ""
url_poster: ""
url_code: ""
---
